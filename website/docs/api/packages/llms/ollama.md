<!-- Code generated by gomarkdoc. DO NOT EDIT -->

# ollama

```go
import "github.com/lookatitude/beluga-ai/llms/ollama"
```

Package ollama provides an implementation of the llms.ChatModel interface using a local Ollama instance.

## Index

- [type OllamaChat](<#OllamaChat>)
  - [func NewOllamaChat\(modelName string, options ...OllamaOption\) \(\*OllamaChat, error\)](<#NewOllamaChat>)
  - [func \(o \*OllamaChat\) Batch\(ctx context.Context, inputs \[\]any, options ...core.Option\) \(\[\]any, error\)](<#OllamaChat.Batch>)
  - [func \(o \*OllamaChat\) BindTools\(toolsToBind \[\]tools.Tool\) llms.ChatModel](<#OllamaChat.BindTools>)
  - [func \(o \*OllamaChat\) Generate\(ctx context.Context, messages \[\]schema.Message, options ...core.Option\) \(schema.Message, error\)](<#OllamaChat.Generate>)
  - [func \(o \*OllamaChat\) Invoke\(ctx context.Context, input any, options ...core.Option\) \(any, error\)](<#OllamaChat.Invoke>)
  - [func \(o \*OllamaChat\) Stream\(ctx context.Context, input any, options ...core.Option\) \(\<\-chan any, error\)](<#OllamaChat.Stream>)
  - [func \(o \*OllamaChat\) StreamChat\(ctx context.Context, messages \[\]schema.Message, options ...core.Option\) \(\<\-chan llms.AIMessageChunk, error\)](<#OllamaChat.StreamChat>)
- [type OllamaOption](<#OllamaOption>)
  - [func WithDefaultOptions\(opts api.Options\) OllamaOption](<#WithDefaultOptions>)
  - [func WithHost\(host string\) OllamaOption](<#WithHost>)
  - [func WithOllamaMaxConcurrentBatches\(n int\) OllamaOption](<#WithOllamaMaxConcurrentBatches>)


<a name="OllamaChat"></a>
## type OllamaChat

OllamaChat represents a chat model client for a local Ollama instance.

```go
type OllamaChat struct {
    // contains filtered or unexported fields
}
```

<a name="NewOllamaChat"></a>
### func NewOllamaChat

```go
func NewOllamaChat(modelName string, options ...OllamaOption) (*OllamaChat, error)
```

NewOllamaChat creates a new Ollama chat client. It requires a model name and accepts functional options for customization.

<a name="OllamaChat.Batch"></a>
### func \(\*OllamaChat\) Batch

```go
func (o *OllamaChat) Batch(ctx context.Context, inputs []any, options ...core.Option) ([]any, error)
```

Batch implements the core.Runnable interface.

<a name="OllamaChat.BindTools"></a>
### func \(\*OllamaChat\) BindTools

```go
func (o *OllamaChat) BindTools(toolsToBind []tools.Tool) llms.ChatModel
```

BindTools implements the llms.ChatModel interface.

<a name="OllamaChat.Generate"></a>
### func \(\*OllamaChat\) Generate

```go
func (o *OllamaChat) Generate(ctx context.Context, messages []schema.Message, options ...core.Option) (schema.Message, error)
```

Generate implements the llms.ChatModel interface.

<a name="OllamaChat.Invoke"></a>
### func \(\*OllamaChat\) Invoke

```go
func (o *OllamaChat) Invoke(ctx context.Context, input any, options ...core.Option) (any, error)
```

Invoke implements the core.Runnable interface.

<a name="OllamaChat.Stream"></a>
### func \(\*OllamaChat\) Stream

```go
func (o *OllamaChat) Stream(ctx context.Context, input any, options ...core.Option) (<-chan any, error)
```

Stream implements the core.Runnable interface.

<a name="OllamaChat.StreamChat"></a>
### func \(\*OllamaChat\) StreamChat

```go
func (o *OllamaChat) StreamChat(ctx context.Context, messages []schema.Message, options ...core.Option) (<-chan llms.AIMessageChunk, error)
```

StreamChat implements the llms.ChatModel interface.

<a name="OllamaOption"></a>
## type OllamaOption

OllamaOption is a function type for setting options on the OllamaChat client.

```go
type OllamaOption func(*OllamaChat)
```

<a name="WithDefaultOptions"></a>
### func WithDefaultOptions

```go
func WithDefaultOptions(opts api.Options) OllamaOption
```

WithDefaultOptions sets default API options for the Ollama client.

<a name="WithHost"></a>
### func WithHost

```go
func WithHost(host string) OllamaOption
```

WithHost sets a custom host URL for the Ollama client.

<a name="WithOllamaMaxConcurrentBatches"></a>
### func WithOllamaMaxConcurrentBatches

```go
func WithOllamaMaxConcurrentBatches(n int) OllamaOption
```

WithOllamaMaxConcurrentBatches sets the concurrency limit for Batch.

Generated by [gomarkdoc](<https://github.com/princjef/gomarkdoc>)
