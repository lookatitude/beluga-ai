<!-- Code generated by gomarkdoc. DO NOT EDIT -->

# openai

```go
import "github.com/lookatitude/beluga-ai/llms/openai"
```

Package openai provides an implementation of the llms.ChatModel interface using the OpenAI API \(including compatible APIs like Azure OpenAI\).

## Index

- [type OpenAIChat](<#OpenAIChat>)
  - [func NewOpenAIChat\(apiKey string, options ...OpenAIClientOption\) \(\*OpenAIChat, error\)](<#NewOpenAIChat>)
  - [func \(o \*OpenAIChat\) Batch\(ctx context.Context, inputs \[\]any, options ...core.Option\) \(\[\]any, error\)](<#OpenAIChat.Batch>)
  - [func \(o \*OpenAIChat\) BindTools\(toolsToBind \[\]tools.Tool\) llms.ChatModel](<#OpenAIChat.BindTools>)
  - [func \(o \*OpenAIChat\) Generate\(ctx context.Context, messages \[\]schema.Message, options ...core.Option\) \(schema.Message, error\)](<#OpenAIChat.Generate>)
  - [func \(o \*OpenAIChat\) Invoke\(ctx context.Context, input any, options ...core.Option\) \(any, error\)](<#OpenAIChat.Invoke>)
  - [func \(o \*OpenAIChat\) Stream\(ctx context.Context, input any, options ...core.Option\) \(\<\-chan any, error\)](<#OpenAIChat.Stream>)
  - [func \(o \*OpenAIChat\) StreamChat\(ctx context.Context, messages \[\]schema.Message, options ...core.Option\) \(\<\-chan llms.AIMessageChunk, error\)](<#OpenAIChat.StreamChat>)
- [type OpenAIClientOption](<#OpenAIClientOption>)
  - [func WithBaseURL\(baseURL string\) OpenAIClientOption](<#WithBaseURL>)
  - [func WithDefaultRequest\(req openai.ChatCompletionRequest\) OpenAIClientOption](<#WithDefaultRequest>)
  - [func WithMaxConcurrentBatches\(n int\) OpenAIClientOption](<#WithMaxConcurrentBatches>)
  - [func WithModel\(modelName string\) OpenAIClientOption](<#WithModel>)


<a name="OpenAIChat"></a>
## type OpenAIChat

OpenAIChat represents a chat model client for OpenAI compatible APIs.

```go
type OpenAIChat struct {
    // contains filtered or unexported fields
}
```

<a name="NewOpenAIChat"></a>
### func NewOpenAIChat

```go
func NewOpenAIChat(apiKey string, options ...OpenAIClientOption) (*OpenAIChat, error)
```

NewOpenAIChat creates a new OpenAI chat client. It requires an API key and accepts functional options for customization.

<a name="OpenAIChat.Batch"></a>
### func \(\*OpenAIChat\) Batch

```go
func (o *OpenAIChat) Batch(ctx context.Context, inputs []any, options ...core.Option) ([]any, error)
```

Batch implements the core.Runnable interface. Executes requests concurrently up to maxConcurrentBatches.

<a name="OpenAIChat.BindTools"></a>
### func \(\*OpenAIChat\) BindTools

```go
func (o *OpenAIChat) BindTools(toolsToBind []tools.Tool) llms.ChatModel
```

BindTools implements the llms.ChatModel interface. It creates a \*new\* OpenAIChat instance with the tools bound.

<a name="OpenAIChat.Generate"></a>
### func \(\*OpenAIChat\) Generate

```go
func (o *OpenAIChat) Generate(ctx context.Context, messages []schema.Message, options ...core.Option) (schema.Message, error)
```

Generate implements the llms.ChatModel interface.

<a name="OpenAIChat.Invoke"></a>
### func \(\*OpenAIChat\) Invoke

```go
func (o *OpenAIChat) Invoke(ctx context.Context, input any, options ...core.Option) (any, error)
```

Invoke implements the core.Runnable interface.

<a name="OpenAIChat.Stream"></a>
### func \(\*OpenAIChat\) Stream

```go
func (o *OpenAIChat) Stream(ctx context.Context, input any, options ...core.Option) (<-chan any, error)
```

Stream implements the core.Runnable interface.

<a name="OpenAIChat.StreamChat"></a>
### func \(\*OpenAIChat\) StreamChat

```go
func (o *OpenAIChat) StreamChat(ctx context.Context, messages []schema.Message, options ...core.Option) (<-chan llms.AIMessageChunk, error)
```

StreamChat implements the llms.ChatModel interface.

<a name="OpenAIClientOption"></a>
## type OpenAIClientOption

OpenAIClientOption is a function type for setting options on the OpenAIChat client.

```go
type OpenAIClientOption func(*OpenAIChat)
```

<a name="WithBaseURL"></a>
### func WithBaseURL

```go
func WithBaseURL(baseURL string) OpenAIClientOption
```

WithBaseURL sets a custom base URL for the OpenAI client \(for Azure, proxies, etc.\).

<a name="WithDefaultRequest"></a>
### func WithDefaultRequest

```go
func WithDefaultRequest(req openai.ChatCompletionRequest) OpenAIClientOption
```

WithDefaultRequest sets default parameters for ChatCompletionRequest.

<a name="WithMaxConcurrentBatches"></a>
### func WithMaxConcurrentBatches

```go
func WithMaxConcurrentBatches(n int) OpenAIClientOption
```

WithMaxConcurrentBatches sets the maximum number of concurrent requests in Batch.

<a name="WithModel"></a>
### func WithModel

```go
func WithModel(modelName string) OpenAIClientOption
```

WithModel sets the default model name for the client.

Generated by [gomarkdoc](<https://github.com/princjef/gomarkdoc>)
