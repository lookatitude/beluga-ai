<!-- Code generated by gomarkdoc. DO NOT EDIT -->

# bedrock

```go
import "github.com/lookatitude/beluga-ai/llms/bedrock"
```

Package bedrock provides an implementation of the llms.ChatModel interface using AWS Bedrock Runtime, with support for multiple model providers.

Package bedrock contains provider\-specific logic for AI21 Labs Jurassic\-2 models on AWS Bedrock.

Package bedrock contains provider\-specific logic for AWS Bedrock.

Package bedrock contains provider\-specific logic for Cohere models on AWS Bedrock.

Package bedrock provides an implementation of the llms.ChatModel interface using AWS Bedrock Runtime, with support for multiple model providers.

Package bedrock provides an implementation of the llms.ChatModel interface using AWS Bedrock Runtime, with support for multiple model providers.

Package bedrock contains provider\-specific logic for Amazon Titan Text models on AWS Bedrock.

## Index

- [Constants](<#constants>)
- [type AI21Completion](<#AI21Completion>)
- [type AI21CompletionData](<#AI21CompletionData>)
- [type AI21FinishReason](<#AI21FinishReason>)
- [type AI21Jurassic2Request](<#AI21Jurassic2Request>)
- [type AI21Jurassic2Response](<#AI21Jurassic2Response>)
- [type AI21Jurassic2StreamResponse](<#AI21Jurassic2StreamResponse>)
- [type AI21Penalty](<#AI21Penalty>)
- [type AI21PromptDetails](<#AI21PromptDetails>)
- [type AI21Token](<#AI21Token>)
- [type BedrockLLM](<#BedrockLLM>)
  - [func NewBedrockLLM\(ctx context.Context, modelID string, options ...BedrockOption\) \(\*BedrockLLM, error\)](<#NewBedrockLLM>)
  - [func \(bl \*BedrockLLM\) Batch\(ctx context.Context, inputs \[\]any, options ...core.Option\) \(\[\]any, error\)](<#BedrockLLM.Batch>)
  - [func \(bl \*BedrockLLM\) BindTools\(toolsToBind \[\]tools.Tool\) llms.ChatModel](<#BedrockLLM.BindTools>)
  - [func \(bl \*BedrockLLM\) Generate\(ctx context.Context, messages \[\]schema.Message, options ...core.Option\) \(schema.Message, error\)](<#BedrockLLM.Generate>)
  - [func \(bl \*BedrockLLM\) Invoke\(ctx context.Context, input any, options ...core.Option\) \(any, error\)](<#BedrockLLM.Invoke>)
  - [func \(bl \*BedrockLLM\) Stream\(ctx context.Context, input any, options ...core.Option\) \(\<\-chan any, error\)](<#BedrockLLM.Stream>)
  - [func \(bl \*BedrockLLM\) StreamChat\(ctx context.Context, messages \[\]schema.Message, options ...core.Option\) \(\<\-chan llms.AIMessageChunk, error\)](<#BedrockLLM.StreamChat>)
- [type BedrockOption](<#BedrockOption>)
  - [func WithBedrockDefaultCallOptions\(opts schema.CallOptions\) BedrockOption](<#WithBedrockDefaultCallOptions>)
  - [func WithBedrockMaxConcurrentBatches\(n int\) BedrockOption](<#WithBedrockMaxConcurrentBatches>)
- [type CohereChatMessage](<#CohereChatMessage>)
- [type CohereCommandRequest](<#CohereCommandRequest>)
- [type CohereCommandResponse](<#CohereCommandResponse>)
- [type CohereGeneration](<#CohereGeneration>)
- [type CohereResponseMeta](<#CohereResponseMeta>)
- [type CohereTool](<#CohereTool>)
- [type CohereToolCall](<#CohereToolCall>)
- [type CohereToolCallResponse](<#CohereToolCallResponse>)
- [type CohereToolParameter](<#CohereToolParameter>)
- [type CohereToolResult](<#CohereToolResult>)
- [type MetaLlamaRequest](<#MetaLlamaRequest>)
- [type MetaLlamaResponse](<#MetaLlamaResponse>)
- [type MetaLlamaStreamResponseChunk](<#MetaLlamaStreamResponseChunk>)
- [type MistralOutput](<#MistralOutput>)
- [type MistralRequest](<#MistralRequest>)
- [type MistralResponse](<#MistralResponse>)
- [type MistralStreamResponseChunk](<#MistralStreamResponseChunk>)
- [type TitanTextConfig](<#TitanTextConfig>)
- [type TitanTextRequest](<#TitanTextRequest>)
- [type TitanTextResponse](<#TitanTextResponse>)
- [type TitanTextResult](<#TitanTextResult>)
- [type TitanTextStreamResponse](<#TitanTextStreamResponse>)


## Constants

<a name="ProviderAnthropic"></a>Constants for known Bedrock model providers

```go
const (
    ProviderAnthropic = "anthropic"
    ProviderMeta      = "meta"
    ProviderCohere    = "cohere"
    ProviderAI21      = "ai21"
    ProviderAmazon    = "amazon" // For Titan models
    ProviderMistral   = "mistral"
)
```

<a name="AI21Completion"></a>
## type AI21Completion

AI21Completion represents a single completion from the AI21 model.

```go
type AI21Completion struct {
    Data         AI21CompletionData `json:"data"`
    FinishReason AI21FinishReason   `json:"finishReason"`
}
```

<a name="AI21CompletionData"></a>
## type AI21CompletionData

AI21CompletionData contains the actual generated text and tokens.

```go
type AI21CompletionData struct {
    Text   string      `json:"text"`
    Tokens []AI21Token `json:"tokens,omitempty"` // If requested
}
```

<a name="AI21FinishReason"></a>
## type AI21FinishReason

AI21FinishReason indicates why the generation stopped.

```go
type AI21FinishReason struct {
    Reason string `json:"reason"` // e.g., "length", "endoftext", "stop"
}
```

<a name="AI21Jurassic2Request"></a>
## type AI21Jurassic2Request

AI21Jurassic2Request represents the request payload for AI21 Jurassic\-2 models on Bedrock. See: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-jurassic2.html

```go
type AI21Jurassic2Request struct {
    Prompt           string       `json:"prompt"`
    MaxTokens        int          `json:"maxTokens,omitempty"`
    Temperature      float64      `json:"temperature,omitempty"` // Changed to float64 for consistency with schema.CallOptions
    TopP             float64      `json:"topP,omitempty"`        // Changed to float64
    StopSequences    []string     `json:"stopSequences,omitempty"`
    CountPenalty     *AI21Penalty `json:"countPenalty,omitempty"`
    PresencePenalty  *AI21Penalty `json:"presencePenalty,omitempty"`
    FrequencyPenalty *AI21Penalty `json:"frequencyPenalty,omitempty"`
    NumResults       int          `json:"numResults,omitempty"` // Default 1
}
```

<a name="AI21Jurassic2Response"></a>
## type AI21Jurassic2Response

AI21Jurassic2Response represents the response payload from AI21 Jurassic\-2 models on Bedrock.

```go
type AI21Jurassic2Response struct {
    ID          string            `json:"id"`
    Prompt      AI21PromptDetails `json:"prompt"`
    Completions []AI21Completion  `json:"completions"`
}
```

<a name="AI21Jurassic2StreamResponse"></a>
## type AI21Jurassic2StreamResponse

AI21Jurassic2StreamResponse represents a chunk in the streaming response. Based on Bedrock documentation, the stream for AI21 provides \`outputText\` and \`amazon\-bedrock\-invocationMetrics\`.

```go
type AI21Jurassic2StreamResponse struct {
    OutputText       string  `json:"outputText,omitempty"`
    CompletionReason *string `json:"completionReason,omitempty"` // Appears in the last chunk
    // Bedrock adds these metrics, typically in the last chunk or a separate metadata chunk.
    InputTokenCount  *int `json:"amazon-bedrock-invocationMetrics_inputTokenCount,omitempty"`
    OutputTokenCount *int `json:"amazon-bedrock-invocationMetrics_outputTokenCount,omitempty"`
}
```

<a name="AI21Penalty"></a>
## type AI21Penalty

AI21Penalty defines penalty structures for AI21 models.

```go
type AI21Penalty struct {
    Scale              float64 `json:"scale"` // Changed to float64
    ApplyToNumbers     bool    `json:"applyToNumbers,omitempty"`
    ApplyToPunctuation bool    `json:"applyToPunctuation,omitempty"`
    ApplyToStopwords   bool    `json:"applyToStopwords,omitempty"`
    ApplyToWhitespaces bool    `json:"applyToWhitespaces,omitempty"`
    ApplyToEmojis      bool    `json:"applyToEmojis,omitempty"`
}
```

<a name="AI21PromptDetails"></a>
## type AI21PromptDetails

AI21PromptDetails contains details about the prompt in the response.

```go
type AI21PromptDetails struct {
    Text   string      `json:"text"`
    Tokens []AI21Token `json:"tokens,omitempty"` // If requested
}
```

<a name="AI21Token"></a>
## type AI21Token

AI21Token represents a token with its text and log probability \(if requested\).

```go
type AI21Token struct {
    GeneratedToken struct {
        Token      string  `json:"token"`
        Logprob    float64 `json:"logprob"`
        RawLogprob float64 `json:"raw_logprob"`
    }   `json:"generatedToken"`
    TopTokens []any `json:"topTokens,omitempty"` // Can be complex, using any
    TextRange struct {
        Start int `json:"start"`
        End   int `json:"end"`
    }   `json:"textRange"`
}
```

<a name="BedrockLLM"></a>
## type BedrockLLM

BedrockLLM represents a chat model client for AWS Bedrock Runtime. It dispatches to provider\-specific logic based on the modelID.

```go
type BedrockLLM struct {
    // contains filtered or unexported fields
}
```

<a name="NewBedrockLLM"></a>
### func NewBedrockLLM

```go
func NewBedrockLLM(ctx context.Context, modelID string, options ...BedrockOption) (*BedrockLLM, error)
```

NewBedrockLLM creates a new Bedrock chat client.

<a name="BedrockLLM.Batch"></a>
### func \(\*BedrockLLM\) Batch

```go
func (bl *BedrockLLM) Batch(ctx context.Context, inputs []any, options ...core.Option) ([]any, error)
```



<a name="BedrockLLM.BindTools"></a>
### func \(\*BedrockLLM\) BindTools

```go
func (bl *BedrockLLM) BindTools(toolsToBind []tools.Tool) llms.ChatModel
```

BindTools implements the llms.ChatModel interface.

<a name="BedrockLLM.Generate"></a>
### func \(\*BedrockLLM\) Generate

```go
func (bl *BedrockLLM) Generate(ctx context.Context, messages []schema.Message, options ...core.Option) (schema.Message, error)
```

Generate implements the llms.ChatModel interface.

<a name="BedrockLLM.Invoke"></a>
### func \(\*BedrockLLM\) Invoke

```go
func (bl *BedrockLLM) Invoke(ctx context.Context, input any, options ...core.Option) (any, error)
```



<a name="BedrockLLM.Stream"></a>
### func \(\*BedrockLLM\) Stream

```go
func (bl *BedrockLLM) Stream(ctx context.Context, input any, options ...core.Option) (<-chan any, error)
```



<a name="BedrockLLM.StreamChat"></a>
### func \(\*BedrockLLM\) StreamChat

```go
func (bl *BedrockLLM) StreamChat(ctx context.Context, messages []schema.Message, options ...core.Option) (<-chan llms.AIMessageChunk, error)
```

StreamChat implements the llms.ChatModel interface.

<a name="BedrockOption"></a>
## type BedrockOption

BedrockOption is a function type for setting options on the BedrockLLM client.

```go
type BedrockOption func(*BedrockLLM)
```

<a name="WithBedrockDefaultCallOptions"></a>
### func WithBedrockDefaultCallOptions

```go
func WithBedrockDefaultCallOptions(opts schema.CallOptions) BedrockOption
```

WithBedrockDefaultCallOptions sets default call options for the Bedrock client.

<a name="WithBedrockMaxConcurrentBatches"></a>
### func WithBedrockMaxConcurrentBatches

```go
func WithBedrockMaxConcurrentBatches(n int) BedrockOption
```

WithBedrockMaxConcurrentBatches sets the concurrency limit for Batch.

<a name="CohereChatMessage"></a>
## type CohereChatMessage

CohereChatMessage represents a message in the chat history for Cohere models.

```go
type CohereChatMessage struct {
    Role    string `json:"role"` // USER, CHATBOT, SYSTEM, TOOL
    Message string `json:"message"`
}
```

<a name="CohereCommandRequest"></a>
## type CohereCommandRequest

CohereCommandRequest represents the request payload for Cohere Command models on Bedrock. See: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-cohere-command.html

```go
type CohereCommandRequest struct {
    Prompt            string              `json:"prompt"`
    MaxTokens         int                 `json:"max_tokens,omitempty"`
    Temperature       float64             `json:"temperature,omitempty"`
    P                 float64             `json:"p,omitempty"` // Nucleus sampling (top-p)
    K                 int                 `json:"k,omitempty"` // Top-k sampling
    StopSequences     []string            `json:"stop_sequences,omitempty"`
    ReturnLikelihoods string              `json:"return_likelihoods,omitempty"` // NONE, GENERATION, ALL
    Stream            bool                `json:"stream,omitempty"`             // Required for streaming
    NumGenerations    int                 `json:"num_generations,omitempty"`    // Not typically used in chat, but available
    LogitBias         map[string]float32  `json:"logit_bias,omitempty"`         // Example: {"234": -5.0}
    Truncate          string              `json:"truncate,omitempty"`           // NONE, START, END
    ChatHistory       []CohereChatMessage `json:"chat_history,omitempty"`
    Tools             []CohereTool        `json:"tools,omitempty"`
    ToolResults       []CohereToolResult  `json:"tool_results,omitempty"`
    ForceSingleStep   bool                `json:"force_single_step,omitempty"` // If true, model makes one tool call and waits for Tool Results
    PromptTruncation  string              `json:"prompt_truncation,omitempty"` // AUTO, AUTO_PRESERVE_ORDER, OFF
}
```

<a name="CohereCommandResponse"></a>
## type CohereCommandResponse

CohereCommandResponse represents the response payload from Cohere Command models on Bedrock.

```go
type CohereCommandResponse struct {
    ID          string             `json:"id,omitempty"`          // Not always present in stream
    Generations []CohereGeneration `json:"generations,omitempty"` // For non-streaming
    Prompt      string             `json:"prompt,omitempty"`      // Echoed prompt
    // Streaming specific fields
    IsFinished   bool   `json:"is_finished,omitempty"`
    FinishReason string `json:"finish_reason,omitempty"` // COMPLETE, ERROR, ERROR_TOXIC, ERROR_LIMIT, USER_CANCEL, MAX_TOKENS, TOOL_CALLS
    Text         string `json:"text,omitempty"`          // For text generation stream event
    // Tool call related fields (can appear in non-streaming or streaming final response)
    ToolCalls   []CohereToolCall    `json:"tool_calls,omitempty"`
    ChatHistory []CohereChatMessage `json:"chat_history,omitempty"` // Updated chat history
    // Meta information, including token counts
    Meta *CohereResponseMeta `json:"meta,omitempty"`
    // Stream event type for differentiating chunks
    EventType string `json:"event_type,omitempty"` // e.g., "text-generation", "tool-calls", "stream-end"
}
```

<a name="CohereGeneration"></a>
## type CohereGeneration

CohereGeneration represents a single generation in a non\-streaming response.

```go
type CohereGeneration struct {
    ID           string              `json:"id"`
    Text         string              `json:"text"`
    FinishReason string              `json:"finish_reason"`
    ToolCalls    []CohereToolCall    `json:"tool_calls,omitempty"`
    ChatHistory  []CohereChatMessage `json:"chat_history,omitempty"` // Bedrock Cohere doesn_t seem to return this in generations
}
```

<a name="CohereResponseMeta"></a>
## type CohereResponseMeta

CohereResponseMeta contains metadata about the response, including token usage.

```go
type CohereResponseMeta struct {
    APIVersion struct {
        Version string `json:"version"`
    }   `json:"api_version"`
    BilledUnits struct {
        InputTokens  int `json:"input_tokens"`
        OutputTokens int `json:"output_tokens"`
    }   `json:"billed_units"`
    Tokens *struct {
        InputTokens  int `json:"input_tokens"`
        OutputTokens int `json:"output_tokens"`
    }   `json:"tokens,omitempty"`
}
```

<a name="CohereTool"></a>
## type CohereTool

CohereTool represents a tool definition for Cohere models.

```go
type CohereTool struct {
    Name                 string                         `json:"name"`
    Description          string                         `json:"description"`
    ParameterDefinitions map[string]CohereToolParameter `json:"parameter_definitions,omitempty"`
}
```

<a name="CohereToolCall"></a>
## type CohereToolCall

CohereToolCall represents a tool call made by the Cohere model \(part of response\).

```go
type CohereToolCall struct {
    Name       string         `json:"name"`
    Parameters map[string]any `json:"parameters"`
}
```

<a name="CohereToolCallResponse"></a>
## type CohereToolCallResponse

CohereToolCallResponse is used within CohereToolResult to identify the call. This mirrors the structure of a tool\_call received from the model.

```go
type CohereToolCallResponse struct {
    Name       string         `json:"name"`
    Parameters map[string]any `json:"parameters"`
}
```

<a name="CohereToolParameter"></a>
## type CohereToolParameter

CohereToolParameter defines a parameter for a Cohere tool.

```go
type CohereToolParameter struct {
    Description string `json:"description,omitempty"`
    Type        string `json:"type"` // string, number, boolean, integer, array, object
    Required    bool   `json:"required,omitempty"`
}
```

<a name="CohereToolResult"></a>
## type CohereToolResult

CohereToolResult represents the result of a tool call to be sent to the model.

```go
type CohereToolResult struct {
    Call    *CohereToolCallResponse `json:"call"`    // Pointer to the tool call that this result corresponds to
    Outputs []map[string]any        `json:"outputs"` // List of outputs, each output is a JSON object
}
```

<a name="MetaLlamaRequest"></a>
## type MetaLlamaRequest

MetaLlamaRequest represents the request payload for Meta Llama models on Bedrock. Based on: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-meta.html

```go
type MetaLlamaRequest struct {
    Prompt      string  `json:"prompt"`
    Temperature float32 `json:"temperature,omitempty"`
    TopP        float32 `json:"top_p,omitempty"`
    MaxGenLen   int     `json:"max_gen_len,omitempty"` // Max tokens to generate

}
```

<a name="MetaLlamaResponse"></a>
## type MetaLlamaResponse

MetaLlamaResponse represents the full response payload from Meta Llama models on Bedrock.

```go
type MetaLlamaResponse struct {
    Generation           string `json:"generation"`
    PromptTokenCount     *int   `json:"prompt_token_count,omitempty"`     // Optional, may not always be present
    GenerationTokenCount *int   `json:"generation_token_count,omitempty"` // Optional
    StopReason           string `json:"stop_reason,omitempty"`            // e.g., "stop", "length", "content_filter"

}
```

<a name="MetaLlamaStreamResponseChunk"></a>
## type MetaLlamaStreamResponseChunk

MetaLlamaStreamResponseChunk represents a chunk in a streaming response from Meta Llama. This is the structure of the JSON object within the \`bytes\` field of brtypes.PayloadPart.

```go
type MetaLlamaStreamResponseChunk struct {
    Generation           string `json:"generation"`                       // The text chunk
    PromptTokenCount     *int   `json:"prompt_token_count,omitempty"`     // Usually in the first chunk or a separate metadata chunk
    GenerationTokenCount *int   `json:"generation_token_count,omitempty"` // Incremental or final count
    StopReason           string `json:"stop_reason,omitempty"`            // Present in the last chunk
    // amazon-bedrock-invocationMetrics might appear in some stream events for token counts
    AmazonBedrockInvocationMetrics *struct {
        InputTokenCount  *int `json:"inputTokenCount"`
        OutputTokenCount *int `json:"outputTokenCount"`
    }   `json:"amazon-bedrock-invocationMetrics,omitempty"`
}
```

<a name="MistralOutput"></a>
## type MistralOutput

MistralOutput represents a single output from the Mistral model.

```go
type MistralOutput struct {
    Text       string `json:"text"`
    StopReason string `json:"stop_reason"` // e.g., "stop", "length", "tool_calls"

}
```

<a name="MistralRequest"></a>
## type MistralRequest

MistralRequest represents the request payload for Mistral models on Bedrock. Based on: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-mistral-chat-completion.html

```go
type MistralRequest struct {
    Prompt      string   `json:"prompt"`
    MaxTokens   int      `json:"max_tokens,omitempty"`
    Temperature float32  `json:"temperature,omitempty"`
    TopP        float32  `json:"top_p,omitempty"`
    TopK        int      `json:"top_k,omitempty"`
    Stop        []string `json:"stop,omitempty"` // Mistral uses "stop"

}
```

<a name="MistralResponse"></a>
## type MistralResponse

MistralResponse represents the response payload from Mistral models on Bedrock.

```go
type MistralResponse struct {
    Outputs []MistralOutput `json:"outputs"`
}
```

<a name="MistralStreamResponseChunk"></a>
## type MistralStreamResponseChunk

MistralStreamResponseChunk represents a chunk in a streaming response from Mistral.

```go
type MistralStreamResponseChunk struct {
    Chunk struct {
        Bytes string `json:"bytes"` // The actual text chunk, often base64 encoded if not text/event-stream
    }   `json:"chunk"` // This structure might vary based on actual Bedrock stream format for Mistral
    // Alternative simpler structure if Bedrock normalizes it:
    Delta struct {
        Text string `json:"text"`
    }   `json:"delta"`
    OutputTokenCount *int `json:"amazon-bedrock-outputTokenCount,omitempty"` // Check if this header is present in stream events

}
```

<a name="TitanTextConfig"></a>
## type TitanTextConfig

TitanTextConfig holds the configuration parameters for Titan Text generation.

```go
type TitanTextConfig struct {
    MaxTokenCount int      `json:"maxTokenCount,omitempty"`
    Temperature   float64  `json:"temperature,omitempty"` // Changed to float64
    TopP          float64  `json:"topP,omitempty"`        // Changed to float64
    StopSequences []string `json:"stopSequences,omitempty"`
}
```

<a name="TitanTextRequest"></a>
## type TitanTextRequest

TitanTextRequest represents the request payload for Amazon Titan Text models \(e.g., titan\-text\-express\-v1, titan\-text\-lite\-v1, titan\-text\-agile\-v1\). See: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-text.html

```go
type TitanTextRequest struct {
    InputText            string           `json:"inputText"`
    TextGenerationConfig *TitanTextConfig `json:"textGenerationConfig,omitempty"`
}
```

<a name="TitanTextResponse"></a>
## type TitanTextResponse

TitanTextResponse represents the response payload from Amazon Titan Text models.

```go
type TitanTextResponse struct {
    InputTextTokenCount int               `json:"inputTextTokenCount"`
    Results             []TitanTextResult `json:"results"`
}
```

<a name="TitanTextResult"></a>
## type TitanTextResult

TitanTextResult represents a single generation result from a Titan Text model.

```go
type TitanTextResult struct {
    TokenCount       int    `json:"tokenCount"`
    OutputText       string `json:"outputText"`
    CompletionReason string `json:"completionReason"` // e.g., "FINISH", "LENGTH", "MAX_TOKENS"
}
```

<a name="TitanTextStreamResponse"></a>
## type TitanTextStreamResponse

TitanTextStreamResponse represents a chunk in the streaming response for Titan Text. Example: \{"outputText": "...", "index": 0, "totalOutputTextTokenCount": null, "completionReason": null, "amazon\-bedrock\-invocationMetrics": \{"inputTokenCount": X, "outputTokenCount": Y\}\}

```go
type TitanTextStreamResponse struct {
    OutputText                string  `json:"outputText,omitempty"`
    Index                     *int    `json:"index,omitempty"`                     // Typically 0 for single generation
    TotalOutputTextTokenCount *int    `json:"totalOutputTextTokenCount,omitempty"` // Null until the end
    CompletionReason          *string `json:"completionReason,omitempty"`          // Null until the end
    InvocationMetrics         *struct {
        InputTokenCount  int `json:"inputTokenCount"`
        OutputTokenCount int `json:"outputTokenCount"`
    }   `json:"amazon-bedrock-invocationMetrics,omitempty"`
}
```

Generated by [gomarkdoc](<https://github.com/princjef/gomarkdoc>)
