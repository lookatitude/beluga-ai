<!-- Code generated by gomarkdoc. DO NOT EDIT -->

# llms

```go
import "github.com/lookatitude/beluga-ai/llms"
```

Package llms defines interfaces for interacting with Large Language Models \(LLMs\) and specific chat models.

## Index

- [func EnsureMessages\(input any\) \(\[\]schema.Message, error\)](<#EnsureMessages>)
- [func GetSystemAndHumanPrompts\(messages \[\]schema.Message\) \(string, string\)](<#GetSystemAndHumanPrompts>)
- [func WithMaxTokens\(tokens int\) core.Option](<#WithMaxTokens>)
- [func WithStopWords\(stop \[\]string\) core.Option](<#WithStopWords>)
- [func WithStreamingFunc\(fn func\(context.Context, AIMessageChunk\) error\) core.Option](<#WithStreamingFunc>)
- [func WithTemperature\(temp float32\) core.Option](<#WithTemperature>)
- [func WithToolChoice\(choice string\) core.Option](<#WithToolChoice>)
- [func WithTools\(toolsToUse \[\]tools.Tool\) core.Option](<#WithTools>)
- [func WithTopK\(topK int\) core.Option](<#WithTopK>)
- [func WithTopP\(topP float32\) core.Option](<#WithTopP>)
- [type AIMessageChunk](<#AIMessageChunk>)
- [type ChatModel](<#ChatModel>)
- [type Option](<#Option>)
  - [func \(o Option\) Apply\(m \*map\[string\]interface\{\}\)](<#Option.Apply>)


<a name="EnsureMessages"></a>
## func EnsureMessages

```go
func EnsureMessages(input any) ([]schema.Message, error)
```

EnsureMessages ensures the input is a slice of schema.Message. It attempts to convert common input types \(like a single string or Message\) into the required format.

<a name="GetSystemAndHumanPrompts"></a>
## func GetSystemAndHumanPrompts

```go
func GetSystemAndHumanPrompts(messages []schema.Message) (string, string)
```

GetSystemAndHumanPrompts extracts the system prompt and concatenates human messages. This is a utility function that might be useful for models that don\_t support distinct system messages or require a single prompt string.

<a name="WithMaxTokens"></a>
## func WithMaxTokens

```go
func WithMaxTokens(tokens int) core.Option
```

WithMaxTokens sets the maximum number of tokens to generate.

<a name="WithStopWords"></a>
## func WithStopWords

```go
func WithStopWords(stop []string) core.Option
```

WithStopWords sets the stop sequences for generation.

<a name="WithStreamingFunc"></a>
## func WithStreamingFunc

```go
func WithStreamingFunc(fn func(context.Context, AIMessageChunk) error) core.Option
```

WithStreamingFunc sets a callback function for streaming responses. The function will be called with each AIMessageChunk. Deprecated: Streaming is handled via the StreamChat method and channels.

<a name="WithTemperature"></a>
## func WithTemperature

```go
func WithTemperature(temp float32) core.Option
```

WithTemperature sets the sampling temperature.

<a name="WithToolChoice"></a>
## func WithToolChoice

```go
func WithToolChoice(choice string) core.Option
```

WithToolChoice forces the model to call a specific tool or no tool. Use tool name to force a specific tool, "any" to allow any tool, "none" to prevent tool use.

<a name="WithTools"></a>
## func WithTools

```go
func WithTools(toolsToUse []tools.Tool) core.Option
```

WithTools sets the tools that the model can call.

<a name="WithTopK"></a>
## func WithTopK

```go
func WithTopK(topK int) core.Option
```

WithTopK sets the top\-k sampling parameter.

<a name="WithTopP"></a>
## func WithTopP

```go
func WithTopP(topP float32) core.Option
```

WithTopP sets the nucleus sampling probability.

<a name="AIMessageChunk"></a>
## type AIMessageChunk

AIMessageChunk represents a chunk of an AI message, typically used in streaming. It can contain content, tool calls, or an error.

```go
type AIMessageChunk struct {
    Content        string // Text content of the chunk
    ToolCallChunks []schema.ToolCallChunk
    AdditionalArgs map[string]interface{} // Provider-specific arguments or metadata
    Err            error                  // Error encountered during streaming for this chunk
}
```

<a name="ChatModel"></a>
## type ChatModel

ChatModel is the interface for chat models. It extends the Runnable interface for more complex interactions.

```go
type ChatModel interface {
    core.Runnable
    // Generate takes a series of messages and returns an AI message.
    // It is a single call to the model.
    Generate(ctx context.Context, messages []schema.Message, options ...core.Option) (schema.Message, error)

    // StreamChat takes a series of messages and returns a channel of AIMessageChunk.
    // This allows for streaming responses from the model.
    StreamChat(ctx context.Context, messages []schema.Message, options ...core.Option) (<-chan AIMessageChunk, error)

    // BindTools binds a list of tools to the ChatModel. The returned ChatModel
    // will then be ableto call these tools.
    // The specific way tools are bound and used depends on the underlying model provider.
    BindTools(toolsToBind []tools.Tool) ChatModel
}
```

<a name="Option"></a>
## type Option

Option is a function type for setting options for LLM calls. It is a wrapper around core.Option for convenience within the llms package. Deprecated: Use core.Option directly.

```go
type Option func(map[string]interface{})
```

<a name="Option.Apply"></a>
### func \(Option\) Apply

```go
func (o Option) Apply(m *map[string]interface{})
```

Apply applies the option to the given map. Deprecated: Use core.Option directly.

Generated by [gomarkdoc](<https://github.com/princjef/gomarkdoc>)
