# Provider Tutorials

Learn how to configure and optimize the underlying AI providers that power Beluga AI.

## Tutorials

### [Adding a New LLM Provider](./llms-new-provider.md)
Step-by-step guide to extending the framework with custom or proprietary LLM backends.

### [Advanced Inference Options](./llms-advanced-inference.md)
Fine-tune generation using temperature, penalties, and sampling strategies.

### [Multimodal Embeddings with Google](./embeddings-multimodal-google.md)
Use Google's advanced embedding models to represent images and text in the same vector space.

### [Fine-tuning Embedding Strategies](./embeddings-finetuning-strategies.md)
Optimize retrieval performance with batching and domain-specific models.

### [Local Development with In-Memory](./vectorstores-inmemory-local.md)
Fast prototyping and testing using high-performance in-memory vector storage.

### [Production pgvector Sharding](./vectorstores-pgvector-sharding.md)
Scale your vector database to millions of entries using PostgreSQL and sharding.

### [Message Template Design](./prompts-message-templates.md)
Create dynamic, reusable prompt templates for complex conversation flows.

### [Reusable System Prompts for Personas](./prompts-reusable-system-prompts.md)
Build a library of agent personas to ensure consistent behavior across your app.
