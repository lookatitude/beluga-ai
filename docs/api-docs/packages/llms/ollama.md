---
title: ollama
sidebar_position: 1
---

<!-- Code generated by gomarkdoc. DO NOT EDIT -->


# ollama

```go
import "github.com/lookatitude/beluga-ai/pkg/llms/providers/ollama"
```

Package ollama provides an implementation of the llms.ChatModel interface using the Ollama API for local LLM models.

## Index

- [Constants](<#constants>)
- [`func NewOllamaProviderFactory() func(*llms.Config) (iface.ChatModel, error)`](<#NewOllamaProviderFactory>)
- [type AdvancedMockOllamaProvider](<#AdvancedMockOllamaProvider>)
  - [`func NewAdvancedMockOllamaProvider(modelName string) *AdvancedMockOllamaProvider`](<#NewAdvancedMockOllamaProvider>)
  - [`func (m *AdvancedMockOllamaProvider) Batch(ctx context.Context, inputs []any, options ...core.Option) ([]any, error)`](<#AdvancedMockOllamaProvider.Batch>)
  - [`func (m *AdvancedMockOllamaProvider) BindTools(toolsToBind []tools.Tool) iface.ChatModel`](<#AdvancedMockOllamaProvider.BindTools>)
  - [`func (m *AdvancedMockOllamaProvider) CheckHealth() map[string]any`](<#AdvancedMockOllamaProvider.CheckHealth>)
  - [`func (m *AdvancedMockOllamaProvider) Generate(ctx context.Context, messages []schema.Message, options ...core.Option) (schema.Message, error)`](<#AdvancedMockOllamaProvider.Generate>)
  - [`func (m *AdvancedMockOllamaProvider) GetCallCount() int`](<#AdvancedMockOllamaProvider.GetCallCount>)
  - [`func (m *AdvancedMockOllamaProvider) GetModelName() string`](<#AdvancedMockOllamaProvider.GetModelName>)
  - [`func (m *AdvancedMockOllamaProvider) GetProviderName() string`](<#AdvancedMockOllamaProvider.GetProviderName>)
  - [`func (m *AdvancedMockOllamaProvider) Invoke(ctx context.Context, input any, options ...core.Option) (any, error)`](<#AdvancedMockOllamaProvider.Invoke>)
  - [`func (m *AdvancedMockOllamaProvider) Reset()`](<#AdvancedMockOllamaProvider.Reset>)
  - [`func (m *AdvancedMockOllamaProvider) SetDelay(delay time.Duration)`](<#AdvancedMockOllamaProvider.SetDelay>)
  - [`func (m *AdvancedMockOllamaProvider) SetError(shouldError bool, err error)`](<#AdvancedMockOllamaProvider.SetError>)
  - [`func (m *AdvancedMockOllamaProvider) SetRateLimit(enabled bool)`](<#AdvancedMockOllamaProvider.SetRateLimit>)
  - [`func (m *AdvancedMockOllamaProvider) Stream(ctx context.Context, input any, options ...core.Option) (<-chan any, error)`](<#AdvancedMockOllamaProvider.Stream>)
  - [`func (m *AdvancedMockOllamaProvider) StreamChat(ctx context.Context, messages []schema.Message, options ...core.Option) (<-chan iface.AIMessageChunk, error)`](<#AdvancedMockOllamaProvider.StreamChat>)
- [type OllamaProvider](<#OllamaProvider>)
  - [`func NewOllamaProvider(config *llms.Config) (*OllamaProvider, error)`](<#NewOllamaProvider>)
  - [`func (o *OllamaProvider) Batch(ctx context.Context, inputs []any, options ...core.Option) ([]any, error)`](<#OllamaProvider.Batch>)
  - [`func (o *OllamaProvider) BindTools(toolsToBind []tools.Tool) iface.ChatModel`](<#OllamaProvider.BindTools>)
  - [`func (o *OllamaProvider) CheckHealth() map[string]any`](<#OllamaProvider.CheckHealth>)
  - [`func (o *OllamaProvider) Generate(ctx context.Context, messages []schema.Message, options ...core.Option) (schema.Message, error)`](<#OllamaProvider.Generate>)
  - [`func (o *OllamaProvider) GetModelName() string`](<#OllamaProvider.GetModelName>)
  - [`func (o *OllamaProvider) GetProviderName() string`](<#OllamaProvider.GetProviderName>)
  - [`func (o *OllamaProvider) Invoke(ctx context.Context, input any, options ...core.Option) (any, error)`](<#OllamaProvider.Invoke>)
  - [`func (o *OllamaProvider) Stream(ctx context.Context, input any, options ...core.Option) (<-chan any, error)`](<#OllamaProvider.Stream>)
  - [`func (o *OllamaProvider) StreamChat(ctx context.Context, messages []schema.Message, options ...core.Option) (<-chan iface.AIMessageChunk, error)`](<#OllamaProvider.StreamChat>)

## Constants

<a name="ProviderName"></a>Provider constants.

```go
const (
    ProviderName = "ollama"
    DefaultModel = "llama2"

    // Error codes specific to Ollama.
    ErrCodeConnectionFailed = "ollama_connection_failed"
    ErrCodeModelNotFound    = "ollama_model_not_found"
    ErrCodeInvalidRequest   = "ollama_invalid_request"
)
```

<a name="NewOllamaProviderFactory"></a>
## func [NewOllamaProviderFactory](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/provider.go#L664>)

```go
func NewOllamaProviderFactory() func(*llms.Config) (iface.ChatModel, error)
```

NewOllamaProviderFactory returns a factory function for creating Ollama providers. This is used for registering the provider with the LLM factory pattern.

Returns:

- func(*llms.Config) (iface.ChatModel, error): Factory function that creates Ollama providers

Example:

```
factory := llms.NewFactory()
factory.RegisterProviderFactory("ollama", ollama.NewOllamaProviderFactory())
provider, err := factory.CreateProvider("ollama", config)
```

Example usage can be found in examples/llm-usage/main.go

<a name="AdvancedMockOllamaProvider"></a>
## type [AdvancedMockOllamaProvider](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/ollama_mock.go#L18-L30>)

AdvancedMockOllamaProvider provides a comprehensive mock implementation for testing Ollama provider.

```go
type AdvancedMockOllamaProvider struct {
    // contains filtered or unexported fields
}
```

<a name="NewAdvancedMockOllamaProvider"></a>
### func [NewAdvancedMockOllamaProvider](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/ollama_mock.go#L33>)

```go
func NewAdvancedMockOllamaProvider(modelName string) *AdvancedMockOllamaProvider
```

NewAdvancedMockOllamaProvider creates a new advanced mock with configurable behavior.

<a name="AdvancedMockOllamaProvider.Batch"></a>
### func (*AdvancedMockOllamaProvider) [Batch](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/ollama_mock.go#L158>)

```go
func (m *AdvancedMockOllamaProvider) Batch(ctx context.Context, inputs []any, options ...core.Option) ([]any, error)
```

Batch implements the Runnable interface.

<a name="AdvancedMockOllamaProvider.BindTools"></a>
### func (*AdvancedMockOllamaProvider) [BindTools](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/ollama_mock.go#L131>)

```go
func (m *AdvancedMockOllamaProvider) BindTools(toolsToBind []tools.Tool) iface.ChatModel
```

BindTools implements the ChatModel interface.

<a name="AdvancedMockOllamaProvider.CheckHealth"></a>
### func (*AdvancedMockOllamaProvider) [CheckHealth](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/ollama_mock.go#L198>)

```go
func (m *AdvancedMockOllamaProvider) CheckHealth() map[string]any
```

CheckHealth implements the ChatModel interface.

<a name="AdvancedMockOllamaProvider.Generate"></a>
### func (*AdvancedMockOllamaProvider) [Generate](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/ollama_mock.go#L47>)

```go
func (m *AdvancedMockOllamaProvider) Generate(ctx context.Context, messages []schema.Message, options ...core.Option) (schema.Message, error)
```

Generate implements the ChatModel interface.

<a name="AdvancedMockOllamaProvider.GetCallCount"></a>
### func (*AdvancedMockOllamaProvider) [GetCallCount](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/ollama_mock.go#L232>)

```go
func (m *AdvancedMockOllamaProvider) GetCallCount() int
```

GetCallCount returns the number of times methods have been called.

<a name="AdvancedMockOllamaProvider.GetModelName"></a>
### func (*AdvancedMockOllamaProvider) [GetModelName](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/ollama_mock.go#L139>)

```go
func (m *AdvancedMockOllamaProvider) GetModelName() string
```

GetModelName implements the ChatModel interface.

<a name="AdvancedMockOllamaProvider.GetProviderName"></a>
### func (*AdvancedMockOllamaProvider) [GetProviderName](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/ollama_mock.go#L144>)

```go
func (m *AdvancedMockOllamaProvider) GetProviderName() string
```

GetProviderName implements the LLM interface.

<a name="AdvancedMockOllamaProvider.Invoke"></a>
### func (*AdvancedMockOllamaProvider) [Invoke](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/ollama_mock.go#L149>)

```go
func (m *AdvancedMockOllamaProvider) Invoke(ctx context.Context, input any, options ...core.Option) (any, error)
```

Invoke implements the Runnable interface.

<a name="AdvancedMockOllamaProvider.Reset"></a>
### func (*AdvancedMockOllamaProvider) [Reset](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/ollama_mock.go#L239>)

```go
func (m *AdvancedMockOllamaProvider) Reset()
```

Reset resets the mock state.

<a name="AdvancedMockOllamaProvider.SetDelay"></a>
### func (*AdvancedMockOllamaProvider) [SetDelay](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/ollama_mock.go#L218>)

```go
func (m *AdvancedMockOllamaProvider) SetDelay(delay time.Duration)
```

SetDelay configures the mock to simulate delay.

<a name="AdvancedMockOllamaProvider.SetError"></a>
### func (*AdvancedMockOllamaProvider) [SetError](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/ollama_mock.go#L210>)

```go
func (m *AdvancedMockOllamaProvider) SetError(shouldError bool, err error)
```

SetError configures the mock to return an error.

<a name="AdvancedMockOllamaProvider.SetRateLimit"></a>
### func (*AdvancedMockOllamaProvider) [SetRateLimit](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/ollama_mock.go#L225>)

```go
func (m *AdvancedMockOllamaProvider) SetRateLimit(enabled bool)
```

SetRateLimit configures the mock to simulate rate limiting.

<a name="AdvancedMockOllamaProvider.Stream"></a>
### func (*AdvancedMockOllamaProvider) [Stream](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/ollama_mock.go#L171>)

```go
func (m *AdvancedMockOllamaProvider) Stream(ctx context.Context, input any, options ...core.Option) (<-chan any, error)
```

Stream implements the Runnable interface.

<a name="AdvancedMockOllamaProvider.StreamChat"></a>
### func (*AdvancedMockOllamaProvider) [StreamChat](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/ollama_mock.go#L84>)

```go
func (m *AdvancedMockOllamaProvider) StreamChat(ctx context.Context, messages []schema.Message, options ...core.Option) (<-chan iface.AIMessageChunk, error)
```

StreamChat implements the ChatModel interface.

<a name="OllamaProvider"></a>
## type [OllamaProvider](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/provider.go#L37-L46>)

OllamaProvider implements the ChatModel interface for Ollama models.

```go
type OllamaProvider struct {
    // contains filtered or unexported fields
}
```

<a name="NewOllamaProvider"></a>
### func [NewOllamaProvider](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/provider.go#L75>)

```go
func NewOllamaProvider(config *llms.Config) (*OllamaProvider, error)
```

NewOllamaProvider creates a new Ollama provider instance. This provider implements the ChatModel interface for local Ollama models (Llama 2, Mistral, etc.). Ollama allows running LLMs locally without API keys.

Parameters:

- config: LLM configuration containing model name and optional base URL

Returns:

- *OllamaProvider: A new Ollama provider instance ready to use
- error: Configuration validation errors or connection errors

Example:

```
config := &llms.Config{
    ModelName: "llama2",
}
// Optional: Set custom base URL
config.ProviderSpecific = map[string]any{
    "base_url": "http://localhost:11434",
}
provider, err := ollama.NewOllamaProvider(config)
if err != nil {
    log.Fatal(err)
}
response, err := provider.Generate(ctx, messages)
```

Example usage can be found in examples/llm-usage/main.go

<a name="OllamaProvider.Batch"></a>
### func (*OllamaProvider) [Batch](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/provider.go#L227>)

```go
func (o *OllamaProvider) Batch(ctx context.Context, inputs []any, options ...core.Option) ([]any, error)
```

Batch implements the Runnable interface.

<a name="OllamaProvider.BindTools"></a>
### func (*OllamaProvider) [BindTools](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/provider.go#L201>)

```go
func (o *OllamaProvider) BindTools(toolsToBind []tools.Tool) iface.ChatModel
```

BindTools implements the ChatModel interface.

<a name="OllamaProvider.CheckHealth"></a>
### func (*OllamaProvider) [CheckHealth](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/provider.go#L640>)

```go
func (o *OllamaProvider) CheckHealth() map[string]any
```

CheckHealth implements the HealthChecker interface.

<a name="OllamaProvider.Generate"></a>
### func (*OllamaProvider) [Generate](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/provider.go#L113>)

```go
func (o *OllamaProvider) Generate(ctx context.Context, messages []schema.Message, options ...core.Option) (schema.Message, error)
```

Generate implements the ChatModel interface.

<a name="OllamaProvider.GetModelName"></a>
### func (*OllamaProvider) [GetModelName](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/provider.go#L209>)

```go
func (o *OllamaProvider) GetModelName() string
```

GetModelName implements the ChatModel interface.

<a name="OllamaProvider.GetProviderName"></a>
### func (*OllamaProvider) [GetProviderName](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/provider.go#L213>)

```go
func (o *OllamaProvider) GetProviderName() string
```

<a name="OllamaProvider.Invoke"></a>
### func (*OllamaProvider) [Invoke](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/provider.go#L218>)

```go
func (o *OllamaProvider) Invoke(ctx context.Context, input any, options ...core.Option) (any, error)
```

Invoke implements the Runnable interface.

<a name="OllamaProvider.Stream"></a>
### func (*OllamaProvider) [Stream](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/provider.go#L267>)

```go
func (o *OllamaProvider) Stream(ctx context.Context, input any, options ...core.Option) (<-chan any, error)
```

Stream implements the Runnable interface.

<a name="OllamaProvider.StreamChat"></a>
### func (*OllamaProvider) [StreamChat](<https://github.com/lookatitude/beluga-ai/blob/main/pkg/llms/providers/ollama/provider.go#L155>)

```go
func (o *OllamaProvider) StreamChat(ctx context.Context, messages []schema.Message, options ...core.Option) (<-chan iface.AIMessageChunk, error)
```

StreamChat implements the ChatModel interface.

Generated by [gomarkdoc](<https://github.com/princjef/gomarkdoc>)
