//go:generate go run github.com/lookatitude/beluga-ai/tools/generate

// Package llms provides code generation utilities for LLM implementations.
// This file contains directives for automated code generation following the
// Beluga AI Framework patterns.
package llms

// Code generation targets:
// 1. Mock implementations for testing
// 2. Provider factory registrations
// 3. Configuration validation code
// 4. Metrics collection boilerplate
// 5. Tracing integration helpers

// MockProviderConfig represents configuration for generating mock providers
type MockProviderConfig struct {
	ProviderName string
	ModelName    string
	Responses    []string
	Errors       []string
	Delay        string
}

// ProviderTemplate represents a template for generating new providers
type ProviderTemplate struct {
	Name       string
	Package    string
	Config     interface{}
	Imports    []string
	Interfaces []string
}

// GenerateMock generates a mock implementation for testing
func GenerateMock(config MockProviderConfig) string {
	template := `// Code generated by Beluga AI Framework - DO NOT EDIT

package mock

import (
	"context"
	"fmt"
	"time"

	"github.com/lookatitude/beluga-ai/pkg/agents/tools"
	"github.com/lookatitude/beluga-ai/pkg/core"
	"github.com/lookatitude/beluga-ai/pkg/llms"
	"github.com/lookatitude/beluga-ai/pkg/llms/iface"
	"github.com/lookatitude/beluga-ai/pkg/schema"
)

// Generated{{.ProviderName}}Mock implements the ChatModel interface for testing
type Generated{{.ProviderName}}Mock struct {
	config    *llms.Config
	modelName string
	responses []string
	callCount int
	shouldErr bool
}

func NewGenerated{{.ProviderName}}Mock(config *llms.Config) (*Generated{{.ProviderName}}Mock, error) {
	return &Generated{{.ProviderName}}Mock{
		config:    config,
		modelName: "{{.ModelName}}",
		responses: []string{{"{"}}{{range .Responses}}"{{.}}", {{end}}{{"}"}},
		callCount: 0,
		shouldErr: false,
	}, nil
}

func (m *Generated{{.ProviderName}}Mock) Generate(ctx context.Context, messages []schema.Message, options ...core.Option) (schema.Message, error) {
	m.callCount++

	if m.shouldErr {
		return nil, fmt.Errorf("mock error")
	}

	response := m.responses[(m.callCount-1)%len(m.responses)]
	return schema.NewAIMessage(response), nil
}

func (m *Generated{{.ProviderName}}Mock) StreamChat(ctx context.Context, messages []schema.Message, options ...core.Option) (<-chan iface.AIMessageChunk, error) {
	outputChan := make(chan iface.AIMessageChunk)

	go func() {
		defer close(outputChan)

		response := m.responses[m.callCount%len(m.responses)]
		chunk := iface.AIMessageChunk{
			Content: response,
		}
		select {
		case outputChan <- chunk:
		case <-ctx.Done():
		}
	}()

	return outputChan, nil
}

func (m *Generated{{.ProviderName}}Mock) BindTools(toolsToBind []tools.Tool) iface.ChatModel {
	newMock := *m
	newMock.tools = make([]tools.Tool, len(toolsToBind))
	copy(newMock.tools, toolsToBind)
	return &newMock
}

func (m *Generated{{.ProviderName}}Mock) GetModelName() string {
	return m.modelName
}

func (m *Generated{{.ProviderName}}Mock) Invoke(ctx context.Context, input any, options ...core.Option) (any, error) {
	messages, err := llms.EnsureMessages(input)
	if err != nil {
		return nil, err
	}
	return m.Generate(ctx, messages, options...)
}

func (m *Generated{{.ProviderName}}Mock) Batch(ctx context.Context, inputs []any, options ...core.Option) ([]any, error) {
	results := make([]any, len(inputs))
	for i, input := range inputs {
		result, err := m.Invoke(ctx, input, options...)
		if err != nil {
			return nil, err
		}
		results[i] = result
	}
	return results, nil
}

func (m *Generated{{.ProviderName}}Mock) Stream(ctx context.Context, input any, options ...core.Option) (<-chan any, error) {
	messages, err := llms.EnsureMessages(input)
	if err != nil {
		return nil, err
	}

	chunkChan, err := m.StreamChat(ctx, messages, options...)
	if err != nil {
		return nil, err
	}

	outputChan := make(chan any)
	go func() {
		defer close(outputChan)
		for chunk := range chunkChan {
			select {
			case outputChan <- chunk:
			case <-ctx.Done():
				return
			}
		}
	}()

	return outputChan, nil
}

func (m *Generated{{.ProviderName}}Mock) GetCallCount() int {
	return m.callCount
}

func (m *Generated{{.ProviderName}}Mock) SetShouldError(shouldErr bool) {
	m.shouldErr = shouldErr
}

func (m *Generated{{.ProviderName}}Mock) Reset() {
	m.callCount = 0
}

// Factory function for creating generated mock providers
func NewGenerated{{.ProviderName}}MockFactory() func(*llms.Config) (iface.ChatModel, error) {
	return func(config *llms.Config) (iface.ChatModel, error) {
		return NewGenerated{{.ProviderName}}Mock(config)
	}
}
`
	return template
}

// GenerateProvider generates a new provider implementation template
func GenerateProvider(template ProviderTemplate) string {
	providerTemplate := `// Code generated by Beluga AI Framework - DO NOT EDIT

package {{.Package}}

import (
	"context"
	"fmt"

	{{range .Imports}}"{{.}}"
	{{end}}
)

// Generated{{.Name}}Provider implements the ChatModel interface for {{.Name}}
type Generated{{.Name}}Provider struct {
	config      *llms.Config
	client      *{{.Name}}Client
	modelName   string
	tools       []tools.Tool
	metrics     llms.MetricsRecorder
	tracing     *llms.TracingHelper
	retryConfig *common.RetryConfig
}

func NewGenerated{{.Name}}Provider(config *llms.Config) (*Generated{{.Name}}Provider, error) {
	if err := llms.ValidateProviderConfig(context.Background(), config); err != nil {
		return nil, fmt.Errorf("invalid {{.Name}} configuration: %w", err)
	}

	modelName := config.ModelName
	if modelName == "" {
		modelName = "{{.Name}}-default-model"
	}

	// Initialize client (placeholder)
	client := &{{.Name}}Client{
		APIKey: config.APIKey,
	}

	provider := &Generated{{.Name}}Provider{
		config:    config,
		client:    client,
		modelName: modelName,
		metrics:   llms.GetMetrics(),
		tracing:   llms.NewTracingHelper(),
		retryConfig: &common.RetryConfig{
			MaxRetries: config.MaxRetries,
			Delay:      config.RetryDelay,
			Backoff:    config.RetryBackoff,
		},
	}

	return provider, nil
}

func (p *Generated{{.Name}}Provider) Generate(ctx context.Context, messages []schema.Message, options ...core.Option) (schema.Message, error) {
	ctx = p.tracing.StartOperation(ctx, "{{.Package}}.generate", "{{.Package}}", p.modelName)
	defer p.tracing.EndSpan(ctx)

	p.metrics.IncrementActiveRequests(ctx, "{{.Package}}", p.modelName)
	defer p.metrics.DecrementActiveRequests(ctx, "{{.Package}}", p.modelName)

	callOpts := p.buildCallOptions(options...)

	var result schema.Message
	var err error

	retryErr := common.RetryWithBackoff(ctx, p.retryConfig, "{{.Package}}.generate", func() error {
		result, err = p.generateInternal(ctx, messages, callOpts)
		return err
	})

	if retryErr != nil {
		p.metrics.RecordError(ctx, "{{.Package}}", p.modelName, llms.GetLLMErrorCode(retryErr))
		p.tracing.RecordError(ctx, retryErr)
		return nil, retryErr
	}

	p.metrics.RecordRequest(ctx, "{{.Package}}", p.modelName, 0)
	return result, nil
}

func (p *Generated{{.Name}}Provider) StreamChat(ctx context.Context, messages []schema.Message, options ...core.Option) (<-chan iface.AIMessageChunk, error) {
	ctx = p.tracing.StartOperation(ctx, "{{.Package}}.stream", "{{.Package}}", p.modelName)
	defer p.tracing.EndSpan(ctx)

	callOpts := p.buildCallOptions(options...)
	return p.streamInternal(ctx, messages, callOpts)
}

func (p *Generated{{.Name}}Provider) BindTools(toolsToBind []tools.Tool) iface.ChatModel {
	newProvider := *p
	newProvider.tools = make([]tools.Tool, len(toolsToBind))
	copy(newProvider.tools, toolsToBind)
	return &newProvider
}

func (p *Generated{{.Name}}Provider) GetModelName() string {
	return p.modelName
}

func (p *Generated{{.Name}}Provider) Invoke(ctx context.Context, input any, options ...core.Option) (any, error) {
	messages, err := llms.EnsureMessages(input)
	if err != nil {
		return nil, err
	}
	return p.Generate(ctx, messages, options...)
}

func (p *Generated{{.Name}}Provider) Batch(ctx context.Context, inputs []any, options ...core.Option) ([]any, error) {
	results := make([]any, len(inputs))
	errors := make([]error, len(inputs))

	sem := make(chan struct{}, p.config.MaxConcurrentBatches)

	for i, input := range inputs {
		sem <- struct{}{}

		go func(index int, currentInput any) {
			defer func() { <-sem }()

			result, err := p.Invoke(ctx, currentInput, options...)
			results[index] = result
			errors[index] = err
		}(i, input)
	}

	for i := 0; i < p.config.MaxConcurrentBatches; i++ {
		sem <- struct{}{}
	}

	var combinedErr error
	for _, err := range errors {
		if err != nil {
			if combinedErr == nil {
				combinedErr = err
			} else {
				combinedErr = fmt.Errorf("%v; %v", combinedErr, err)
			}
		}
	}

	return results, combinedErr
}

func (p *Generated{{.Name}}Provider) Stream(ctx context.Context, input any, options ...core.Option) (<-chan any, error) {
	messages, err := llms.EnsureMessages(input)
	if err != nil {
		return nil, err
	}

	chunkChan, err := p.StreamChat(ctx, messages, options...)
	if err != nil {
		return nil, err
	}

	outputChan := make(chan any)
	go func() {
		defer close(outputChan)
		for chunk := range chunkChan {
			select {
			case outputChan <- chunk:
			case <-ctx.Done():
				return
			}
		}
	}()

	return outputChan, nil
}

// Internal implementation methods (placeholders)
func (p *Generated{{.Name}}Provider) generateInternal(ctx context.Context, messages []schema.Message, opts *llms.CallOptions) (schema.Message, error) {
	// TODO: Implement {{.Name}} API call
	return schema.NewAIMessage("Generated response from {{.Name}}"), nil
}

func (p *Generated{{.Name}}Provider) streamInternal(ctx context.Context, messages []schema.Message, opts *llms.CallOptions) (<-chan iface.AIMessageChunk, error) {
	// TODO: Implement {{.Name}} streaming API call
	outputChan := make(chan iface.AIMessageChunk, 1)
	outputChan <- iface.AIMessageChunk{
		Content: "Generated streaming response from {{.Name}}",
	}
	close(outputChan)
	return outputChan, nil
}

func (p *Generated{{.Name}}Provider) buildCallOptions(options ...core.Option) *llms.CallOptions {
	callOpts := llms.NewCallOptions()

	if p.config.MaxTokens != nil {
		callOpts.MaxTokens = p.config.MaxTokens
	}
	if p.config.Temperature != nil {
		temp := float32(*p.config.Temperature)
		callOpts.Temperature = &temp
	}

	for _, opt := range options {
		callOpts.ApplyCallOption(opt)
	}

	return callOpts
}

// {{.Name}}Client represents the API client (placeholder)
type {{.Name}}Client struct {
	APIKey string
	BaseURL string
}

// Factory function for creating generated {{.Package}} providers
func NewGenerated{{.Name}}ProviderFactory() func(*llms.Config) (iface.ChatModel, error) {
	return func(config *llms.Config) (iface.ChatModel, error) {
		return NewGenerated{{.Name}}Provider(config)
	}
}
`
	return providerTemplate
}

// GenerateMetrics generates metrics collection code
func GenerateMetrics() string {
	metricsTemplate := `// Code generated by Beluga AI Framework - DO NOT EDIT

package llms

import (
	"context"
	"time"

	"go.opentelemetry.io/otel/metric"
)

// GeneratedMetrics provides OpenTelemetry-based metrics collection
type GeneratedMetrics struct {
	meter                    metric.Meter
	requestsTotal           metric.Int64Counter
	requestsByProvider      metric.Int64Counter
	requestsByModel         metric.Int64Counter
	requestDuration         metric.Float64Histogram
	errorsTotal             metric.Int64Counter
	errorsByProvider        metric.Int64Counter
	errorsByCode            metric.Int64Counter
	tokenUsageTotal         metric.Int64Counter
	toolCallsTotal          metric.Int64Counter
	activeRequests          metric.Int64UpDownCounter
	activeStreams           metric.Int64UpDownCounter
	streamDuration          metric.Float64Histogram
	batchSize               metric.Int64Histogram
	batchDuration           metric.Float64Histogram
}

func NewGeneratedMetrics(meter metric.Meter) *GeneratedMetrics {
	return &GeneratedMetrics{
		meter: meter,
		requestsTotal: metric.Must(meter).NewInt64Counter(
			"llm_requests_total",
			metric.WithDescription("Total number of LLM requests"),
		),
		requestsByProvider: metric.Must(meter).NewInt64Counter(
			"llm_requests_by_provider_total",
			metric.WithDescription("Total number of LLM requests by provider"),
		),
		requestsByModel: metric.Must(meter).NewInt64Counter(
			"llm_requests_by_model_total",
			metric.WithDescription("Total number of LLM requests by model"),
		),
		requestDuration: metric.Must(meter).NewFloat64Histogram(
			"llm_request_duration_seconds",
			metric.WithDescription("Duration of LLM requests"),
		),
		errorsTotal: metric.Must(meter).NewInt64Counter(
			"llm_errors_total",
			metric.WithDescription("Total number of LLM errors"),
		),
		errorsByProvider: metric.Must(meter).NewInt64Counter(
			"llm_errors_by_provider_total",
			metric.WithDescription("Total number of LLM errors by provider"),
		),
		errorsByCode: metric.Must(meter).NewInt64Counter(
			"llm_errors_by_code_total",
			metric.WithDescription("Total number of LLM errors by error code"),
		),
		tokenUsageTotal: metric.Must(meter).NewInt64Counter(
			"llm_token_usage_total",
			metric.WithDescription("Total token usage"),
		),
		toolCallsTotal: metric.Must(meter).NewInt64Counter(
			"llm_tool_calls_total",
			metric.WithDescription("Total number of tool calls"),
		),
		activeRequests: metric.Must(meter).NewInt64UpDownCounter(
			"llm_active_requests",
			metric.WithDescription("Number of active LLM requests"),
		),
		activeStreams: metric.Must(meter).NewInt64UpDownCounter(
			"llm_active_streams",
			metric.WithDescription("Number of active LLM streams"),
		),
		streamDuration: metric.Must(meter).NewFloat64Histogram(
			"llm_stream_duration_seconds",
			metric.WithDescription("Duration of LLM streams"),
		),
		batchSize: metric.Must(meter).NewInt64Histogram(
			"llm_batch_size",
			metric.WithDescription("Size of LLM batches"),
		),
		batchDuration: metric.Must(meter).NewFloat64Histogram(
			"llm_batch_duration_seconds",
			metric.WithDescription("Duration of LLM batches"),
		),
	}
}

func (m *GeneratedMetrics) RecordRequest(ctx context.Context, provider, model string, duration time.Duration) {
	m.requestsTotal.Add(ctx, 1)
	m.requestsByProvider.Add(ctx, 1,
		metric.WithAttributes(metric.String("provider", provider)))
	m.requestsByModel.Add(ctx, 1,
		metric.WithAttributes(metric.String("model", model)))
	m.requestDuration.Record(ctx, duration.Seconds(),
		metric.WithAttributes(
			metric.String("provider", provider),
			metric.String("model", model),
		))
}

func (m *GeneratedMetrics) RecordError(ctx context.Context, provider, model, errorCode string) {
	m.errorsTotal.Add(ctx, 1)
	m.errorsByProvider.Add(ctx, 1,
		metric.WithAttributes(metric.String("provider", provider)))
	m.errorsByCode.Add(ctx, 1,
		metric.WithAttributes(metric.String("error_code", errorCode)))
}

func (m *GeneratedMetrics) RecordTokenUsage(ctx context.Context, provider, model string, inputTokens, outputTokens int) {
	totalTokens := inputTokens + outputTokens
	m.tokenUsageTotal.Add(ctx, int64(totalTokens))
}

func (m *GeneratedMetrics) RecordToolCall(ctx context.Context, provider, model, toolName string) {
	m.toolCallsTotal.Add(ctx, 1,
		metric.WithAttributes(
			metric.String("provider", provider),
			metric.String("model", model),
			metric.String("tool", toolName),
		))
}

func (m *GeneratedMetrics) RecordBatch(ctx context.Context, provider, model string, batchSize int, duration time.Duration) {
	m.batchSize.Record(ctx, int64(batchSize))
	m.batchDuration.Record(ctx, duration.Seconds(),
		metric.WithAttributes(
			metric.String("provider", provider),
			metric.String("model", model),
		))
}

func (m *GeneratedMetrics) RecordStream(ctx context.Context, provider, model string, duration time.Duration) {
	m.streamDuration.Record(ctx, duration.Seconds(),
		metric.WithAttributes(
			metric.String("provider", provider),
			metric.String("model", model),
		))
}

func (m *GeneratedMetrics) IncrementActiveRequests(ctx context.Context, provider, model string) {
	m.activeRequests.Add(ctx, 1,
		metric.WithAttributes(
			metric.String("provider", provider),
			metric.String("model", model),
		))
}

func (m *GeneratedMetrics) DecrementActiveRequests(ctx context.Context, provider, model string) {
	m.activeRequests.Add(ctx, -1,
		metric.WithAttributes(
			metric.String("provider", provider),
			metric.String("model", model),
		))
}

func (m *GeneratedMetrics) IncrementActiveStreams(ctx context.Context, provider, model string) {
	m.activeStreams.Add(ctx, 1,
		metric.WithAttributes(
			metric.String("provider", provider),
			metric.String("model", model),
		))
}

func (m *GeneratedMetrics) DecrementActiveStreams(ctx context.Context, provider, model string) {
	m.activeStreams.Add(ctx, -1,
		metric.WithAttributes(
			metric.String("provider", provider),
			metric.String("model", model),
		))
}
`
	return metricsTemplate
}
